{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "JRNTF-yGVCey"
      },
      "outputs": [],
      "source": [
        "!pip install -q contractions scikit-learn Sastrawi googletrans==4.0.0-rc1 langdetect pandas matplotlib yfinance tensorflow xgboost\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "import contractions\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "from langdetect import detect\n",
        "from googletrans import Translator\n",
        "from sklearn.metrics import r2_score\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from math import sqrt\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import TimeSeriesSplit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "mtVQ0e22z5Hl"
      },
      "outputs": [],
      "source": [
        "# Load  dataset\n",
        "url = 'https://raw.githubusercontent.com/22bayusetia/PyCuan/main/Sentiment%20Analysis/data_finance.csv'\n",
        "# url = 'https://raw.githubusercontent.com/hairulysin/Algoritma_Academy/main/dataset.csv'\n",
        "df = pd.read_csv(url, delimiter=',', encoding='latin-1', header=None)\n",
        "df = df.drop(0)\n",
        "df.columns = ['label', 'en_text', 'id_text']\n",
        "df = df[['label', 'id_text', 'en_text']]\n",
        "# df.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load stopwords for Indonesian\n",
        "indonesian_stopwords = set(nltk.corpus.stopwords.words('indonesian'))\n",
        "\n",
        "# Load Sastrawi stemmer and stopword remover\n",
        "factory1 = StopWordRemoverFactory()\n",
        "stopword_sastrawi = factory1.create_stop_word_remover()\n",
        "\n",
        "factory2 = StemmerFactory()\n",
        "stemmer_sastrawi = factory2.create_stemmer()\n",
        "\n",
        "# Data preprocessing functions\n",
        "\n",
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    [s.extract() for s in soup(['iframe', 'script'])]\n",
        "    stripped_text = soup.get_text()\n",
        "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
        "    return stripped_text\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text\n",
        "\n",
        "def stopwords_removal(words, language):\n",
        "    if language == 'english':\n",
        "        list_stopwords = nltk.corpus.stopwords.words('english')\n",
        "    elif language == 'indonesian':\n",
        "        list_stopwords = indonesian_stopwords\n",
        "    return [word for word in words if word not in list_stopwords]\n",
        "\n",
        "def preprocess_text_sastrawi(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [stopword_sastrawi.remove(token) for token in tokens]\n",
        "    # tokens = [stemmer_sastrawi.stem(token) for token in tokens if token != '']\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def pre_process_text(text, language):\n",
        "    text = text.lower()\n",
        "    text = strip_html_tags(text)\n",
        "    text = text.translate(text.maketrans(\"\\n\\t\\r\", \"   \"))\n",
        "    text = remove_accented_chars(text)\n",
        "    text = contractions.fix(text)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text, re.I | re.A)\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    text = preprocess_text_sastrawi(text) if language == 'indonesian' else text\n",
        "    return text\n",
        "\n",
        "# Apply data preprocessing\n",
        "df['en_text'] = df['en_text'].apply(lambda x: pre_process_text(x, 'english'))\n",
        "df['id_text'] = df['id_text'].apply(lambda x: pre_process_text(x, 'indonesian'))\n",
        "df = df.drop_duplicates(subset=['en_text', 'id_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGce2DZSVwJV",
        "outputId": "3c483748-4386-452b-c6af-5a0c284227d8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "<ipython-input-32-fd4633374d41>:17: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_sentiment_analysis(df):\n",
        "    # Upsample the minority class after train-validation-test split\n",
        "    data_majority = df[df['label'] == \"positive\"]\n",
        "    data_minority = df[df['label'] == \"negative\"]\n",
        "\n",
        "    data_minority_upsampled = resample(data_minority,\n",
        "                                       replace=True,\n",
        "                                       n_samples=data_majority.shape[0],\n",
        "                                       random_state=123)\n",
        "\n",
        "    df_balance_upsampled = pd.concat([data_majority, data_minority_upsampled])\n",
        "    df_balanced_upsampled = df.drop_duplicates(subset=['en_text', 'id_text'])\n",
        "\n",
        "    # Split data (80:10:10)\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(df_balance_upsampled.en_text,\n",
        "                                                        df_balance_upsampled.label,\n",
        "                                                        test_size=0.2,\n",
        "                                                        random_state=42)\n",
        "\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "    # Convert text sequences to dense feature vectors using TF-IDF\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=3000)\n",
        "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "    X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
        "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "    # Initialize the Random Forest classifier\n",
        "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "    # Train the Random Forest classifier with validation set\n",
        "    rf_classifier.fit(X_train_tfidf, y_train)\n",
        "    y_pred = rf_classifier.predict(X_test_tfidf)\n",
        "\n",
        "    # Evaluate the model on training, validation, and test sets\n",
        "    train_acc = rf_classifier.score(X_train_tfidf, y_train)\n",
        "    val_acc = rf_classifier.score(X_val_tfidf, y_val)\n",
        "    test_acc = rf_classifier.score(X_test_tfidf, y_test)\n",
        "\n",
        "    # Metrics Evaluation\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    # Sentiment Prediction Distribution\n",
        "    predicted_sentiments = rf_classifier.predict(X_test_tfidf)\n",
        "    positive_percentage = (predicted_sentiments == 'positive').sum() / len(predicted_sentiments) * 100\n",
        "    negative_percentage = 100 - positive_percentage\n",
        "\n",
        "    return train_acc, val_acc, test_acc, precision, recall, f1, positive_percentage, negative_percentage\n",
        "\n",
        "sentiment_results = perform_sentiment_analysis(df)"
      ],
      "metadata": {
        "id": "4ee2b4a_Zvo2"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from math import sqrt\n",
        "\n",
        "# Function for time series forecasting\n",
        "def perform_time_series_forecasting(df, stock_symbol, start_date, end_date, seq_length=30, forecast_days=5):\n",
        "    # Download historical stock data\n",
        "    df_stock = yf.download(stock_symbol, start=start_date, end=end_date)\n",
        "    ts = df_stock['Open'].values\n",
        "\n",
        "    # Standardize the time series data\n",
        "    scaler = StandardScaler()\n",
        "    ts_scaled = scaler.fit_transform(np.array(ts).reshape(-1, 1))\n",
        "\n",
        "    # Prepare training data\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "\n",
        "    for i in range(len(ts_scaled) - seq_length):\n",
        "        X_train.append(ts_scaled[i:i + seq_length])\n",
        "        y_train.append(ts_scaled[i + seq_length])\n",
        "\n",
        "    X_train = np.array(X_train)\n",
        "    y_train = np.array(y_train)\n",
        "\n",
        "    train_size = int(len(X_train) * 0.8)\n",
        "    X_train, X_test = X_train[:train_size], X_train[train_size:]\n",
        "    y_train, y_test = y_train[:train_size], y_train[train_size:]\n",
        "\n",
        "    # Build and train LSTM model\n",
        "    model = keras.Sequential()\n",
        "    model.add(LSTM(128, activation='relu', return_sequences=True, input_shape=(seq_length, 1)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(128, activation='relu', return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(128, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "    # Generate forecast for the next 'forecast_days'\n",
        "    X_forecast = np.copy(X_test[-1])\n",
        "    forecasted_values = []\n",
        "\n",
        "    for _ in range(forecast_days):\n",
        "        forecasted_value = model.predict(X_forecast.reshape(1, seq_length, 1))\n",
        "        forecasted_values.append(forecasted_value[0, 0])\n",
        "\n",
        "        X_forecast = np.roll(X_forecast, -1)\n",
        "        X_forecast[-1] = forecasted_value\n",
        "\n",
        "    forecasted_values = scaler.inverse_transform(np.array(forecasted_values).reshape(-1, 1))\n",
        "\n",
        "    # Evaluate the forecasting results\n",
        "    rmse_test = sqrt(mean_squared_error(y_test, model.predict(X_test)))\n",
        "    mae_test = mean_absolute_error(y_test, model.predict(X_test))\n",
        "\n",
        "    # Return the forecasting results\n",
        "    weighted_metric = (rmse_test + mae_test) / 2\n",
        "    last_date = df_stock.index[-1]\n",
        "    forecast_dates = pd.date_range(last_date, periods=forecast_days + 1)[1:]\n",
        "\n",
        "    return weighted_metric, forecast_dates, forecasted_values\n",
        "\n",
        "# Definisi variabel-variabel\n",
        "stock_symbol = 'AAPL'\n",
        "start_date = '2022-01-01'\n",
        "end_date = '2022-12-31'\n",
        "\n",
        "# Panggil fungsi perform_time_series_forecasting dengan data frame df\n",
        "time_series_results = perform_time_series_forecasting(df, stock_symbol, start_date, end_date)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUDLsRLZmCGU",
        "outputId": "e25134f7-72ac-4366-fd4d-6659fc91bfe8"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "5/5 [==============================] - 7s 351ms/step - loss: 0.8883 - val_loss: 0.5183\n",
            "Epoch 2/50\n",
            "5/5 [==============================] - 1s 137ms/step - loss: 0.6953 - val_loss: 0.5198\n",
            "Epoch 3/50\n",
            "5/5 [==============================] - 1s 136ms/step - loss: 0.5450 - val_loss: 0.3362\n",
            "Epoch 4/50\n",
            "5/5 [==============================] - 1s 131ms/step - loss: 0.4810 - val_loss: 0.4355\n",
            "Epoch 5/50\n",
            "5/5 [==============================] - 1s 137ms/step - loss: 0.4286 - val_loss: 0.1930\n",
            "Epoch 6/50\n",
            "5/5 [==============================] - 1s 139ms/step - loss: 0.3763 - val_loss: 0.2725\n",
            "Epoch 7/50\n",
            "5/5 [==============================] - 1s 240ms/step - loss: 0.3034 - val_loss: 0.1314\n",
            "Epoch 8/50\n",
            "5/5 [==============================] - 1s 133ms/step - loss: 0.2723 - val_loss: 0.1354\n",
            "Epoch 9/50\n",
            "5/5 [==============================] - 1s 131ms/step - loss: 0.2546 - val_loss: 0.1326\n",
            "Epoch 10/50\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 0.2502 - val_loss: 0.1040\n",
            "Epoch 11/50\n",
            "5/5 [==============================] - 1s 132ms/step - loss: 0.2434 - val_loss: 0.1643\n",
            "Epoch 12/50\n",
            "5/5 [==============================] - 1s 136ms/step - loss: 0.2568 - val_loss: 0.1254\n",
            "Epoch 13/50\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.2391 - val_loss: 0.1153\n",
            "Epoch 14/50\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 0.1880 - val_loss: 0.1154\n",
            "Epoch 15/50\n",
            "5/5 [==============================] - 1s 224ms/step - loss: 0.1771 - val_loss: 0.1444\n",
            "Epoch 16/50\n",
            "5/5 [==============================] - 1s 252ms/step - loss: 0.2046 - val_loss: 0.1026\n",
            "Epoch 17/50\n",
            "5/5 [==============================] - 1s 251ms/step - loss: 0.1768 - val_loss: 0.1010\n",
            "Epoch 18/50\n",
            "5/5 [==============================] - 1s 238ms/step - loss: 0.2083 - val_loss: 0.0957\n",
            "Epoch 19/50\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.1751 - val_loss: 0.1169\n",
            "Epoch 20/50\n",
            "5/5 [==============================] - 1s 131ms/step - loss: 0.1814 - val_loss: 0.1008\n",
            "Epoch 21/50\n",
            "5/5 [==============================] - 1s 140ms/step - loss: 0.1741 - val_loss: 0.1006\n",
            "Epoch 22/50\n",
            "5/5 [==============================] - 1s 139ms/step - loss: 0.1660 - val_loss: 0.1081\n",
            "Epoch 23/50\n",
            "5/5 [==============================] - 1s 133ms/step - loss: 0.1738 - val_loss: 0.0903\n",
            "Epoch 24/50\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 0.1363 - val_loss: 0.0855\n",
            "Epoch 25/50\n",
            "5/5 [==============================] - 1s 140ms/step - loss: 0.1584 - val_loss: 0.0874\n",
            "Epoch 26/50\n",
            "5/5 [==============================] - 1s 132ms/step - loss: 0.1525 - val_loss: 0.0943\n",
            "Epoch 27/50\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 0.1668 - val_loss: 0.1000\n",
            "Epoch 28/50\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 0.1600 - val_loss: 0.1027\n",
            "Epoch 29/50\n",
            "5/5 [==============================] - 1s 130ms/step - loss: 0.1445 - val_loss: 0.0860\n",
            "Epoch 30/50\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 0.1534 - val_loss: 0.0870\n",
            "Epoch 31/50\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.1394 - val_loss: 0.0981\n",
            "Epoch 32/50\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 0.1576 - val_loss: 0.0810\n",
            "Epoch 33/50\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 0.1335 - val_loss: 0.0849\n",
            "Epoch 34/50\n",
            "5/5 [==============================] - 1s 253ms/step - loss: 0.1438 - val_loss: 0.0865\n",
            "Epoch 35/50\n",
            "5/5 [==============================] - 1s 245ms/step - loss: 0.1467 - val_loss: 0.0813\n",
            "Epoch 36/50\n",
            "5/5 [==============================] - 1s 253ms/step - loss: 0.1430 - val_loss: 0.0823\n",
            "Epoch 37/50\n",
            "5/5 [==============================] - 1s 228ms/step - loss: 0.1386 - val_loss: 0.0849\n",
            "Epoch 38/50\n",
            "5/5 [==============================] - 1s 250ms/step - loss: 0.1540 - val_loss: 0.0853\n",
            "Epoch 39/50\n",
            "5/5 [==============================] - 1s 248ms/step - loss: 0.1440 - val_loss: 0.0779\n",
            "Epoch 40/50\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 0.1410 - val_loss: 0.0851\n",
            "Epoch 41/50\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 0.1447 - val_loss: 0.1132\n",
            "Epoch 42/50\n",
            "5/5 [==============================] - 1s 138ms/step - loss: 0.1383 - val_loss: 0.0808\n",
            "Epoch 43/50\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 0.1459 - val_loss: 0.0796\n",
            "Epoch 44/50\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 0.1358 - val_loss: 0.1028\n",
            "Epoch 45/50\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 0.1232 - val_loss: 0.0818\n",
            "Epoch 46/50\n",
            "5/5 [==============================] - 1s 132ms/step - loss: 0.1293 - val_loss: 0.0796\n",
            "Epoch 47/50\n",
            "5/5 [==============================] - 1s 138ms/step - loss: 0.1229 - val_loss: 0.0829\n",
            "Epoch 48/50\n",
            "5/5 [==============================] - 1s 137ms/step - loss: 0.1174 - val_loss: 0.0799\n",
            "Epoch 49/50\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 0.1197 - val_loss: 0.0791\n",
            "1/1 [==============================] - 0s 422ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "2/2 [==============================] - 0s 57ms/step\n",
            "2/2 [==============================] - 0s 57ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_weights(sentiment_weight, time_series_weight, sentiment_weight_ratio=0.65):\n",
        "    combined_weight = (sentiment_weight_ratio * sentiment_weight) + ((1 - sentiment_weight_ratio) * time_series_weight)\n",
        "    return combined_weight\n",
        "\n",
        "sentiment_weight = perform_sentiment_analysis(df)\n",
        "\n",
        "time_series_weight, _, _ = perform_time_series_forecasting(df, stock_symbol, start_date, end_date)\n",
        "\n",
        "final_weight = combine_weights(sentiment_weight, time_series_weight)\n",
        "\n",
        "print(\"Weight from Sentiment Analysis:\", sentiment_weight)\n",
        "print(\"Weight from Time Series Forecasting:\", time_series_weight)\n",
        "print(\"Combined Weight:\", final_weight)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0erTYgpZ3va",
        "outputId": "e0bf4000-a8b0-44f4-a389-cb4e4b0729b5"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed\n",
            "Epoch 1/50\n",
            "5/5 [==============================] - 5s 243ms/step - loss: 0.8871 - val_loss: 0.4508\n",
            "Epoch 2/50\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.7017 - val_loss: 0.4939\n",
            "Epoch 3/50\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 0.5471 - val_loss: 0.4194\n",
            "Epoch 4/50\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 0.4780 - val_loss: 0.2598\n",
            "Epoch 5/50\n",
            "5/5 [==============================] - 1s 137ms/step - loss: 0.3816 - val_loss: 0.3170\n",
            "Epoch 6/50\n",
            "5/5 [==============================] - 1s 137ms/step - loss: 0.3232 - val_loss: 0.1806\n",
            "Epoch 7/50\n",
            "5/5 [==============================] - 1s 133ms/step - loss: 0.3084 - val_loss: 0.1074\n",
            "Epoch 8/50\n",
            "5/5 [==============================] - 1s 131ms/step - loss: 0.2650 - val_loss: 0.1700\n",
            "Epoch 9/50\n",
            "5/5 [==============================] - 1s 234ms/step - loss: 0.2669 - val_loss: 0.1120\n",
            "Epoch 10/50\n",
            "5/5 [==============================] - 1s 232ms/step - loss: 0.2243 - val_loss: 0.1054\n",
            "Epoch 11/50\n",
            "5/5 [==============================] - 1s 237ms/step - loss: 0.2205 - val_loss: 0.1123\n",
            "Epoch 12/50\n",
            "5/5 [==============================] - 1s 226ms/step - loss: 0.1979 - val_loss: 0.1662\n",
            "Epoch 13/50\n",
            "5/5 [==============================] - 1s 129ms/step - loss: 0.2568 - val_loss: 0.1018\n",
            "Epoch 14/50\n",
            "5/5 [==============================] - 1s 131ms/step - loss: 0.2561 - val_loss: 0.0985\n",
            "Epoch 15/50\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.2204 - val_loss: 0.1591\n",
            "Epoch 16/50\n",
            "5/5 [==============================] - 1s 133ms/step - loss: 0.1932 - val_loss: 0.1079\n",
            "Epoch 17/50\n",
            "5/5 [==============================] - 1s 132ms/step - loss: 0.2051 - val_loss: 0.1030\n",
            "Epoch 18/50\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.1994 - val_loss: 0.0997\n",
            "Epoch 19/50\n",
            "5/5 [==============================] - 1s 133ms/step - loss: 0.1765 - val_loss: 0.1012\n",
            "Epoch 20/50\n",
            "5/5 [==============================] - 1s 132ms/step - loss: 0.1665 - val_loss: 0.0987\n",
            "Epoch 21/50\n",
            "5/5 [==============================] - 1s 136ms/step - loss: 0.1384 - val_loss: 0.1030\n",
            "Epoch 22/50\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 0.1560 - val_loss: 0.0858\n",
            "Epoch 23/50\n",
            "5/5 [==============================] - 1s 132ms/step - loss: 0.1355 - val_loss: 0.0827\n",
            "Epoch 24/50\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 0.1517 - val_loss: 0.0911\n",
            "Epoch 25/50\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 0.1580 - val_loss: 0.1120\n",
            "Epoch 26/50\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 0.1636 - val_loss: 0.0872\n",
            "Epoch 27/50\n",
            "5/5 [==============================] - 1s 243ms/step - loss: 0.1827 - val_loss: 0.0902\n",
            "Epoch 28/50\n",
            "5/5 [==============================] - 2s 335ms/step - loss: 0.1689 - val_loss: 0.0971\n",
            "Epoch 29/50\n",
            "5/5 [==============================] - 2s 334ms/step - loss: 0.1531 - val_loss: 0.0793\n",
            "Epoch 30/50\n",
            "5/5 [==============================] - 1s 275ms/step - loss: 0.1386 - val_loss: 0.1107\n",
            "Epoch 31/50\n",
            "5/5 [==============================] - 1s 217ms/step - loss: 0.1461 - val_loss: 0.0925\n",
            "Epoch 32/50\n",
            "5/5 [==============================] - 1s 239ms/step - loss: 0.1518 - val_loss: 0.0783\n",
            "Epoch 33/50\n",
            "5/5 [==============================] - 1s 200ms/step - loss: 0.1470 - val_loss: 0.0781\n",
            "Epoch 34/50\n",
            "5/5 [==============================] - 1s 227ms/step - loss: 0.1333 - val_loss: 0.0852\n",
            "Epoch 35/50\n",
            "5/5 [==============================] - 1s 212ms/step - loss: 0.1445 - val_loss: 0.0775\n",
            "Epoch 36/50\n",
            "5/5 [==============================] - 1s 206ms/step - loss: 0.1301 - val_loss: 0.0834\n",
            "Epoch 37/50\n",
            "5/5 [==============================] - 1s 239ms/step - loss: 0.1380 - val_loss: 0.0911\n",
            "Epoch 38/50\n",
            "5/5 [==============================] - 1s 195ms/step - loss: 0.1393 - val_loss: 0.0961\n",
            "Epoch 39/50\n",
            "5/5 [==============================] - 1s 254ms/step - loss: 0.1481 - val_loss: 0.0823\n",
            "Epoch 40/50\n",
            "5/5 [==============================] - 1s 300ms/step - loss: 0.1172 - val_loss: 0.0843\n",
            "Epoch 41/50\n",
            "5/5 [==============================] - 2s 357ms/step - loss: 0.1269 - val_loss: 0.0862\n",
            "Epoch 42/50\n",
            "5/5 [==============================] - 2s 316ms/step - loss: 0.1417 - val_loss: 0.0817\n",
            "Epoch 43/50\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.1166 - val_loss: 0.1155\n",
            "Epoch 44/50\n",
            "5/5 [==============================] - 1s 133ms/step - loss: 0.1296 - val_loss: 0.0886\n",
            "Epoch 45/50\n",
            "5/5 [==============================] - 1s 136ms/step - loss: 0.1237 - val_loss: 0.0784\n",
            "1/1 [==============================] - 0s 406ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "2/2 [==============================] - 0s 27ms/step\n",
            "2/2 [==============================] - 0s 27ms/step\n",
            "Weight from Sentiment Analysis: 0.984287283837274\n",
            "Weight from Time Series Forecasting: 0.365977414918195\n",
            "Combined Weight: 0.7678788297155964\n"
          ]
        }
      ]
    }
  ]
}