{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E56bMe2MltWE",
        "outputId": "4d9f172e-c0a8-47d2-bd00-2bc55205619a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Sentiment Analysis Dependencies\n",
        "!pip install -q contractions scikit-learn Sastrawi googletrans==4.0.0-rc1 langdetect gdown\n",
        "import joblib\n",
        "from google.colab import files\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "import re\n",
        "import unicodedata\n",
        "from googletrans import Translator\n",
        "import contractions\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Time Series Dependencies\n",
        "import gdown\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import yfinance as yf\n",
        "\n",
        "def download_sentiment_models():\n",
        "    file_id_tfidf = '134JrTPXdmm6lXZH84ZGk-xsQsEyvvnrL'\n",
        "    file_id_rf = '1zhdKOAbGP_wsQRRrhbbxRjQ6Wep_3LKu'\n",
        "    url_tfidf = f'https://drive.google.com/uc?id={file_id_tfidf}'\n",
        "    url_rf = f'https://drive.google.com/uc?id={file_id_rf}'\n",
        "\n",
        "    output_tfidf = 'tfidf_vectorizer.joblib'\n",
        "    output_rf = 'random_forest_model.joblib'\n",
        "\n",
        "    gdown.download(url_tfidf, output_tfidf, quiet=False)\n",
        "    gdown.download(url_rf, output_rf, quiet=False)\n",
        "\n",
        "def download_time_series_model():\n",
        "    file_id_time_series = '1hQqkpeXQOXH79bNCin1o4So1vGdl3Ent'\n",
        "    url_time_series = f'https://drive.google.com/uc?id={file_id_time_series}'\n",
        "    output_time_series = 'time_series_model.h5'\n",
        "\n",
        "    gdown.download(url_time_series, output_time_series, quiet=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Input Data**"
      ],
      "metadata": {
        "id": "35F6wWESuFbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = \"Revision of Subsidized Fertilizer Policy, Now Farmers Can Redeem Using KTP\"\n",
        "\n",
        "stock_symbol = 'FTT-USD'\n",
        "start_date = '2022-11-14'\n",
        "end_date = '2023-11-14'"
      ],
      "metadata": {
        "id": "BC4dAXe6shJF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi-fungsi pra-pemrosesan teks\n",
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    [s.extract() for s in soup(['iframe', 'script'])]\n",
        "    stripped_text = soup.get_text()\n",
        "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
        "    return stripped_text\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text\n",
        "\n",
        "def pre_process_text(text, language):\n",
        "    text = text.lower()\n",
        "    text = strip_html_tags(text)\n",
        "    text = text.translate(text.maketrans(\"\\n\\t\\r\", \"   \"))\n",
        "    text = remove_accented_chars(text)\n",
        "    text = contractions.fix(text)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text, re.I | re.A)\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    if language == 'indonesian':\n",
        "        text = preprocess_text_sastrawi(text)\n",
        "    return text\n",
        "\n",
        "# Fungsi pra-pemrosesan teks khusus Bahasa Indonesia\n",
        "def preprocess_text_sastrawi(text):\n",
        "    factory1 = StopWordRemoverFactory()\n",
        "    stopword_sastrawi = factory1.create_stop_word_remover()\n",
        "\n",
        "    factory2 = StemmerFactory()\n",
        "    stemmer_sastrawi = factory2.create_stemmer()\n",
        "\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [stopword_sastrawi.remove(token) for token in tokens]\n",
        "    tokens = [stemmer_sastrawi.stem(token) for token in tokens if token != '']\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Load the models\n",
        "tfidf_vectorizer = joblib.load('/content/tfidf_vectorizer.joblib')\n",
        "rf_classifier = joblib.load('/content/random_forest_model.joblib')\n",
        "\n",
        "# Preprocess the new text\n",
        "preprocessed_text = pre_process_text(new_text, 'indonesian')\n",
        "\n",
        "# Mengonversi teks yang telah di-preprocess menjadi fitur TF-IDF\n",
        "new_text_tfidf = tfidf_vectorizer.transform([preprocessed_text])\n",
        "\n",
        "# Melakukan prediksi sentimen menggunakan model Random Forest\n",
        "predicted_label = rf_classifier.predict(new_text_tfidf)\n",
        "\n",
        "translator = Translator()\n",
        "translated_text = translator.translate(new_text, dest='en').text\n",
        "\n",
        "# Mengonversi teks yang telah diterjemahkan menjadi fitur TF-IDF\n",
        "translated_text_tfidf = tfidf_vectorizer.transform([translated_text])\n",
        "\n",
        "# Menampilkan prediksi sentimen\n",
        "predicted_sentiment = rf_classifier.predict(translated_text_tfidf)\n",
        "sentiment_probability = rf_classifier.predict_proba(translated_text_tfidf)[0, 1]\n",
        "\n",
        "threshold = 0.5  # Threshold bisa diatur sesuai kebutuhan\n",
        "sentiment = \"Positive\" if sentiment_probability > threshold else \"Negative\"\n",
        "\n",
        "# Time Series Analysis\n",
        "\n",
        "# Get historical stock data\n",
        "new_df = yf.download(stock_symbol, start=start_date, end=end_date)\n",
        "\n",
        "# Pilih kolom 'Open' (butuhnya opening price)\n",
        "new_ts = new_df['Open'].values\n",
        "\n",
        "# Normalisasi data\n",
        "scaler = StandardScaler()\n",
        "new_data_normalized = scaler.fit_transform(np.array(new_ts).reshape(-1, 1))\n",
        "\n",
        "# Ensure the new data is in the sequence format similar to training data\n",
        "seq_length = 30\n",
        "\n",
        "# Prepare X_new_data\n",
        "X_new_data = []\n",
        "\n",
        "for i in range(len(new_data_normalized) - seq_length):\n",
        "    X_new_data.append(new_data_normalized[i:i + seq_length])\n",
        "\n",
        "# Convert X_new_data to numpy array\n",
        "X_new_data = np.array(X_new_data)\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the pre-trained time series analysis model\n",
        "model = load_model('/content/time_series_model.h5')\n",
        "\n",
        "predictions = model.predict(X_new_data)\n",
        "\n",
        "# forecasting\n",
        "forecast_days = 5\n",
        "X_forecast = np.copy(new_data_normalized[-seq_length:])\n",
        "\n",
        "forecasted_values = []\n",
        "for _ in range(forecast_days):\n",
        "    forecasted_value = model.predict(X_forecast.reshape(1, seq_length, 1))\n",
        "    forecasted_values.append(forecasted_value[0, 0])\n",
        "\n",
        "    X_forecast = np.roll(X_forecast, -1)\n",
        "    X_forecast[-1] = forecasted_value\n",
        "\n",
        "last_actual_day = new_df.index[-1]  # Last day of the actual data\n",
        "forecast_dates = pd.date_range(last_actual_day, periods=forecast_days + 1)[1:]\n",
        "\n",
        "last_actual_opening_price = new_df['Open'][-1]  # Opening price of the last day in the actual data\n",
        "first_forecast_opening_price = forecasted_values[0]  # Opening price of the first day in the forecast\n",
        "\n",
        "price_difference = first_forecast_opening_price - last_actual_opening_price\n",
        "percentage_change = price_difference / last_actual_opening_price\n",
        "\n",
        "print(f\"last actual opening price: {last_actual_day} = {last_actual_opening_price}\")\n",
        "print(f\"first forecast opening price: {forecast_dates[0]} = {first_forecast_opening_price}\")\n",
        "print(f\"Difference in opening stock price between last actual day and first forecast day: {price_difference}\")\n",
        "print(f\"Percentage Change: {percentage_change*100}%\")\n",
        "\n",
        "# adjusted percentage_change to be weighted metric\n",
        "weighted_metric = (percentage_change + 1) / 2\n",
        "print(f\"\\nweighted metric: {weighted_metric}\")\n",
        "\n",
        "time_series_weight = weighted_metric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bKNfWPNpcaZ",
        "outputId": "dd094df8-cba3-46cc-c48e-4b0d2a087618"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*********************100%%**********************]  1 of 1 completed\n",
            "11/11 [==============================] - 1s 74ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "last actual opening price: 2023-11-13 00:00:00 = 3.4254369735717773\n",
            "first forecast opening price: 2023-11-14 00:00:00 = 1.612433671951294\n",
            "Difference in opening stock price between last actual day and first forecast day: -1.8130033016204834\n",
            "Percentage Change: -52.92765027085071%\n",
            "\n",
            "weighted metric: 0.23536174864574644\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk mengkombinasikan bobot\n",
        "def combine_weights(sentiment_probability, time_series_weight, sentiment_ratio=0.65):\n",
        "    time_series_ratio = 1 - sentiment_ratio\n",
        "\n",
        "    combined_weight = (sentiment_ratio * sentiment_probability + time_series_ratio * time_series_weight)\n",
        "    return combined_weight\n",
        "\n",
        "final_weight = combine_weights(sentiment_probability, time_series_weight)\n",
        "final_sentiment = \"Positive📈\" if final_weight > 0.5 else \"Negative📉\"\n",
        "\n",
        "print(\"Bobot:\", final_weight)\n",
        "print(\"Sentiment:\", final_sentiment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwAPOgK5nIDz",
        "outputId": "e1f6e427-f03c-4bc7-850d-31046e3dc274"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bobot: 0.5113766120260113\n",
            "Sentiment: Positive📈\n"
          ]
        }
      ]
    }
  ]
}